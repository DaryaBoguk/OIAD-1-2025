import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv(r'C:\Users\Asus\fldr\datasets\students_simple.csv')

x = df.iloc[:, 1].values  # —Å—Ç–æ–ª–±–µ—Ü ‚Ññ2
y = df.iloc[:, 8].values  # —Å—Ç–æ–ª–±–µ—Ü ‚Ññ9

# 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
def fechner_corr(x, y): #—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–Ω–∞–∫–æ–≤ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ.
    x_sign = np.sign(x - np.mean(x))
    y_sign = np.sign(y - np.mean(y))
    matches = np.sum(x_sign == y_sign)
    mismatches = np.sum(x_sign != y_sign)
    return (matches - mismatches) / len(x)

fechner = fechner_corr(x, y) # —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∑–Ω–∞–∫–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ.
pearson, p_value = stats.pearsonr(x, y) #–ª–∏–Ω–µ–π–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏.
spearman, _ = stats.spearmanr(x, y) #–º–æ–Ω–æ—Ç–æ–Ω–Ω—É—é  –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å.
kendall, _ = stats.kendalltau(x, y) #–°—á–∏—Ç–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä –∏–¥—É—Ç –≤ –æ–¥–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —á—ë–∏ —Å–∫–æ–ª—å–∫–æ ‚Äî –≤ —Ä–∞–∑–Ω—ã—Ö.

# –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –ü–∏—Ä—Å–æ–Ω–∞ -–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω,
#  –≤ –∫–æ—Ç–æ—Ä–æ–º —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 
# –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∏—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–π —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏.
def pearson_ci(r, n, alpha=0.05):
    z = np.arctanh(r)
    se = 1 / np.sqrt(n - 3)
    z_crit = stats.norm.ppf(1 - alpha / 2)
    z_interval = [z - z_crit * se, z + z_crit * se]
    return np.tanh(z_interval)

ci_low, ci_high = pearson_ci(pearson, len(x))

print(f"–§–µ—Ö–Ω–µ—Ä: {fechner:.3f}")
print(f"–ü–∏—Ä—Å–æ–Ω: {pearson:.3f}, p={p_value:.3f}, CI=({ci_low:.3f}, {ci_high:.3f})")
print(f"–°–ø–∏—Ä–º–µ–Ω: {spearman:.3f}")
print(f"–ö–µ–Ω–¥–µ–ª–ª: {kendall:.3f}")

# 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(x, kde=True)
plt.title("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ X")
plt.subplot(1, 2, 2)
sns.histplot(y, kde=True)
plt.title("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ Y")
plt.show()

plt.figure(figsize=(6, 5))
sns.scatterplot(x=x, y=y)
plt.title("–ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å—Å–µ—è–Ω–∏—è")
plt.xlabel("X")
plt.ylabel("Y")
plt.show()

# 3. –†–µ–≥—Ä–µ—Å—Å–∏–∏
def plot_regression(model_name, y_pred):
    plt.figure(figsize=(6, 5))
    sns.scatterplot(x=x, y=y, label='–î–∞–Ω–Ω—ã–µ')
    sns.lineplot(x=x, y=y_pred, color='red', label=model_name)
    plt.title(f"{model_name} —Ä–µ–≥—Ä–µ—Å—Å–∏—è")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()
    plt.show()

# –õ–∏–Ω–µ–π–Ω–∞—è = ùë¶ = w1x + w0
lin_model = LinearRegression()
x_lin = x.reshape(-1, 1)
lin_model.fit(x_lin, y)
y_lin_pred = lin_model.predict(x_lin)
r2_lin = r2_score(y, y_lin_pred)
plot_regression("–õ–∏–Ω–µ–π–Ω–∞—è", y_lin_pred)

# –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è = w2x^2 + w1x + w0
poly = PolynomialFeatures(degree=2)
x_quad = poly.fit_transform(x_lin)
quad_model = LinearRegression()
quad_model.fit(x_quad, y)
y_quad_pred = quad_model.predict(x_quad)
r2_quad = r2_score(y, y_quad_pred)
plot_regression("–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è", y_quad_pred)

# –ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è: y = a/x + b
x_hyp = 1 / x_lin
hyp_model = LinearRegression()
hyp_model.fit(x_hyp, y)
y_hyp_pred = hyp_model.predict(x_hyp)
r2_hyp = r2_score(y, y_hyp_pred)
plot_regression("–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è", y_hyp_pred)

# –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è: y = a * exp(bx) ‚Üí ln(y) = bx + ln(a)
x_exp = x_lin
y_log = np.log(y)
exp_model = LinearRegression()
exp_model.fit(x_exp, y_log)
y_exp_pred = np.exp(exp_model.predict(x_exp))
r2_exp = r2_score(y, y_exp_pred)
plot_regression("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è", y_exp_pred)

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é –§–∏—à–µ—Ä–∞
def fisher_test(r2, n, k):
    F = (r2 / (1 - r2)) * ((n - k) / (k - 1))
    p = 1 - stats.f.cdf(F, k - 1, n - k)
    return F, p

models_r2 = {
    "–õ–∏–Ω–µ–π–Ω–∞—è": (r2_lin, 2),
    "–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è": (r2_quad, 3),
    "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è": (r2_hyp, 2),
    "–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è": (r2_exp, 2)
}

best_model = max(models_r2.items(), key=lambda x: x[1][0])
worst_model = min(models_r2.items(), key=lambda x: x[1][0])

for name, (r2, k) in [best_model, worst_model]:
    F, p = fisher_test(r2, len(x), k)
    print(f"{name} –º–æ–¥–µ–ª—å: F={F:.2f}, p={p:.3f}")

# 5. –í—ã–≤–æ–¥—ã
print("\n–í—ã–≤–æ–¥—ã:")
print("- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å—Ç–µ–ø–µ–Ω—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏, –Ω–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω–∞—è ‚Äî –ü–∏—Ä—Å–æ–Ω–∞.")
print("- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.")
print(f"- –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model[0]} (R¬≤={best_model[1][0]:.3f})")
print(f"- –•—É–¥—à–∞—è –º–æ–¥–µ–ª—å: {worst_model[0]} (R¬≤={worst_model[1][0]:.3f})")
print("- F-–∫—Ä–∏—Ç–µ—Ä–∏–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏.")
